{"cells":[{"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n","This starter notebook is provided by the Keras team.</center>\n","\n","## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n","\n","In this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n","A dataset of 10,000 tweets that were hand classified is available. \n","\n","__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n","\n","\n","**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n","\n","The BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n","\n","BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n","\n","**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n","\n","\n","\n","![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n","\n","\n","\n","In this notebook, you will:\n","\n","- Load the Disaster Tweets\n","- Explore the dataset\n","- Preprocess the data\n","- Load a DistilBERT model from Keras NLP\n","- Train your own model, fine-tuning BERT\n","- Generate the submission file\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-31T20:00:45.041191Z","iopub.status.busy":"2023-07-31T20:00:45.040776Z","iopub.status.idle":"2023-07-31T20:01:10.624768Z","shell.execute_reply":"2023-07-31T20:01:10.623539Z","shell.execute_reply.started":"2023-07-31T20:00:45.041149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras-core in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.4)\n","Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (1.4.0)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (1.24.3)\n","Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (13.5.2)\n","Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (0.0.7)\n","Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (3.9.0)\n","Requirement already satisfied: dm-tree in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras-core) (0.1.8)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich->keras-core) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/martinbravodiaz/Library/Python/3.11/lib/python/site-packages (from rich->keras-core) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\n"]}],"source":["!pip install keras-core --upgrade\n","!pip install -q keras-nlp --upgrade\n","\n","# This sample uses Keras Core, the multi-backend version of Keras.\n","# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\n","import os\n","os.environ['KERAS_BACKEND'] = 'tensorflow'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:10.62922Z","iopub.status.busy":"2023-07-31T20:01:10.628924Z","iopub.status.idle":"2023-07-31T20:01:20.556625Z","shell.execute_reply":"2023-07-31T20:01:20.555665Z","shell.execute_reply.started":"2023-07-31T20:01:10.629194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using TensorFlow backend\n","TensorFlow version: 2.13.0\n","KerasNLP version: 0.6.1\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import tensorflow as tf\n","import keras_core as keras\n","import keras_nlp\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"KerasNLP version:\", keras_nlp.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Load the Disaster Tweets\n","Let's have a look at the train and test dataset.\n","\n","They contain:\n","- id\n","- keyword: A keyword from that tweet (although this may be blank!)\n","- location: The location the tweet was sent from (may also be blank)\n","- text: The text of a tweet\n","- target: 1 if the tweet is a real disaster or 0 if not"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.558472Z","iopub.status.busy":"2023-07-31T20:01:20.557857Z","iopub.status.idle":"2023-07-31T20:01:20.62669Z","shell.execute_reply":"2023-07-31T20:01:20.625756Z","shell.execute_reply.started":"2023-07-31T20:01:20.558435Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Shape = (7613, 5)\n","Training Set Memory Usage = 0.29 MB\n","Test Set Shape = (3263, 4)\n","Test Set Memory Usage = 0.10 MB\n"]}],"source":["df_train = pd.read_csv(\"train.csv\")\n","df_test = pd.read_csv(\"test.csv\")\n","\n","print('Training Set Shape = {}'.format(df_train.shape))\n","print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n","print('Test Set Shape = {}'.format(df_test.shape))\n","print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.629925Z","iopub.status.busy":"2023-07-31T20:01:20.629651Z","iopub.status.idle":"2023-07-31T20:01:20.646214Z","shell.execute_reply":"2023-07-31T20:01:20.645286Z","shell.execute_reply.started":"2023-07-31T20:01:20.6299Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.649765Z","iopub.status.busy":"2023-07-31T20:01:20.649504Z","iopub.status.idle":"2023-07-31T20:01:20.659375Z","shell.execute_reply":"2023-07-31T20:01:20.658296Z","shell.execute_reply.started":"2023-07-31T20:01:20.649741Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Explore the dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.661278Z","iopub.status.busy":"2023-07-31T20:01:20.660532Z","iopub.status.idle":"2023-07-31T20:01:20.686752Z","shell.execute_reply":"2023-07-31T20:01:20.685851Z","shell.execute_reply.started":"2023-07-31T20:01:20.661241Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Length Stat\n","count    7613.000000\n","mean      101.037436\n","std        33.781325\n","min         7.000000\n","25%        78.000000\n","50%       107.000000\n","75%       133.000000\n","max       157.000000\n","Name: length, dtype: float64\n","\n","Test Length Stat\n","count    3263.000000\n","mean      102.108183\n","std        33.972158\n","min         5.000000\n","25%        78.000000\n","50%       109.000000\n","75%       134.000000\n","max       151.000000\n","Name: length, dtype: float64\n"]}],"source":["df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n","df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n","\n","print(\"Train Length Stat\")\n","print(df_train[\"length\"].describe())\n","print()\n","\n","print(\"Test Length Stat\")\n","print(df_test[\"length\"].describe())"]},{"cell_type":"markdown","metadata":{},"source":["If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n","\n","Note that all the tweets are in english."]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess the data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.688711Z","iopub.status.busy":"2023-07-31T20:01:20.688377Z","iopub.status.idle":"2023-07-31T20:01:20.694731Z","shell.execute_reply":"2023-07-31T20:01:20.693638Z","shell.execute_reply.started":"2023-07-31T20:01:20.688679Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","NUM_TRAINING_EXAMPLES = df_train.shape[0]\n","TRAIN_SPLIT = 0.8\n","VAL_SPLIT = 0.2\n","STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n","\n","EPOCHS = 2\n","AUTO = tf.data.experimental.AUTOTUNE"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.697023Z","iopub.status.busy":"2023-07-31T20:01:20.696116Z","iopub.status.idle":"2023-07-31T20:01:20.717538Z","shell.execute_reply":"2023-07-31T20:01:20.716665Z","shell.execute_reply.started":"2023-07-31T20:01:20.696979Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_train[\"text\"]\n","y = df_train[\"target\"]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n","\n","X_test = df_test[\"text\"]"]},{"cell_type":"markdown","metadata":{},"source":["# Load a DistilBERT model from Keras NLP\n","\n","Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n","\n","The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n","\n","We will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n","\n","It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n","\n","Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:20.719649Z","iopub.status.busy":"2023-07-31T20:01:20.719008Z","iopub.status.idle":"2023-07-31T20:01:37.14406Z","shell.execute_reply":"2023-07-31T20:01:37.142842Z","shell.execute_reply.started":"2023-07-31T20:01:20.7196Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/keras-nlp/models/distil_bert_base_en_uncased/v1/vocab.txt\n"]},{"ename":"Exception","evalue":"URL fetch failure on https://storage.googleapis.com/keras-nlp/models/distil_bert_base_en_uncased/v1/vocab.txt: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1458\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1459\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    525\u001b[0m     )\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n","\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_core/src/utils/file_utils.py:284\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[1;32m    285\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)>","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m preset\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdistil_bert_base_en_uncased\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Use a shorter sequence length.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m preprocessor \u001b[39m=\u001b[39m keras_nlp\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mDistilBertPreprocessor\u001b[39m.\u001b[39;49mfrom_preset(preset,\n\u001b[1;32m      6\u001b[0m                                                                    sequence_length\u001b[39m=\u001b[39;49m\u001b[39m160\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                                                                    name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_4_tweets\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m                                                                   )\n\u001b[1;32m     10\u001b[0m \u001b[39m# Pretrained classifier.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m classifier \u001b[39m=\u001b[39m keras_nlp\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mDistilBertClassifier\u001b[39m.\u001b[39mfrom_preset(preset,\n\u001b[1;32m     12\u001b[0m                                                                preprocessor \u001b[39m=\u001b[39m preprocessor, \n\u001b[1;32m     13\u001b[0m                                                                num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_nlp/src/models/preprocessor.py:133\u001b[0m, in \u001b[0;36mPreprocessor.__init_subclass__.<locals>.from_preset\u001b[0;34m(calling_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_preset\u001b[39m(calling_cls, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mcls\u001b[39;49m, calling_cls)\u001b[39m.\u001b[39;49mfrom_preset(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_nlp/src/models/preprocessor.py:95\u001b[0m, in \u001b[0;36mPreprocessor.from_preset\u001b[0;34m(cls, preset, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m preset \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpresets:\n\u001b[1;32m     90\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`preset` must be one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpresets)\u001b[39m}\u001b[39;00m\u001b[39m. Received: \u001b[39m\u001b[39m{\u001b[39;00mpreset\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer_cls\u001b[39m.\u001b[39;49mfrom_preset(preset)\n\u001b[1;32m     97\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpresets[preset]\n\u001b[1;32m     98\u001b[0m \u001b[39m# For task model presets, the backbone config is nested.\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_nlp/src/tokenizers/word_piece_tokenizer.py:503\u001b[0m, in \u001b[0;36mWordPieceTokenizer.__init_subclass__.<locals>.from_preset\u001b[0;34m(calling_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_preset\u001b[39m(calling_cls, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mcls\u001b[39;49m, calling_cls)\u001b[39m.\u001b[39;49mfrom_preset(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_nlp/src/tokenizers/word_piece_tokenizer.py:478\u001b[0m, in \u001b[0;36mWordPieceTokenizer.from_preset\u001b[0;34m(cls, preset, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    473\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`preset` must be one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpresets)\u001b[39m}\u001b[39;00m\u001b[39m. Received: \u001b[39m\u001b[39m{\u001b[39;00mpreset\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    475\u001b[0m     )\n\u001b[1;32m    476\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpresets[preset]\n\u001b[0;32m--> 478\u001b[0m vocabulary \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_file(\n\u001b[1;32m    479\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mvocab.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    480\u001b[0m     metadata[\u001b[39m\"\u001b[39;49m\u001b[39mvocabulary_url\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    481\u001b[0m     cache_subdir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\"\u001b[39;49m, preset),\n\u001b[1;32m    482\u001b[0m     file_hash\u001b[39m=\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mvocabulary_hash\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    485\u001b[0m config \u001b[39m=\u001b[39m metadata[\u001b[39m\"\u001b[39m\u001b[39mpreprocessor_config\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    486\u001b[0m config\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    487\u001b[0m     {\n\u001b[1;32m    488\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvocabulary\u001b[39m\u001b[39m\"\u001b[39m: vocabulary,\n\u001b[1;32m    489\u001b[0m     },\n\u001b[1;32m    490\u001b[0m )\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras_core/src/utils/file_utils.py:288\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n\u001b[1;32m    287\u001b[0m     \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39merrno, e\u001b[39m.\u001b[39mreason))\n\u001b[1;32m    289\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fpath):\n","\u001b[0;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/keras-nlp/models/distil_bert_base_en_uncased/v1/vocab.txt: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)"]}],"source":["# Load a DistilBERT model.\n","preset= \"distil_bert_base_en_uncased\"\n","\n","# Use a shorter sequence length.\n","preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n","                                                                   sequence_length=160,\n","                                                                   name=\"preprocessor_4_tweets\"\n","                                                                  )\n","\n","# Pretrained classifier.\n","classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n","                                                               preprocessor = preprocessor, \n","                                                               num_classes=2)\n","\n","classifier.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Train your own model, fine-tuning BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:01:37.146295Z","iopub.status.busy":"2023-07-31T20:01:37.145916Z","iopub.status.idle":"2023-07-31T20:04:19.910429Z","shell.execute_reply":"2023-07-31T20:04:19.909408Z","shell.execute_reply.started":"2023-07-31T20:01:37.146259Z"},"trusted":true},"outputs":[],"source":["# Compile\n","classifier.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n","    optimizer=keras.optimizers.Adam(1e-5),\n","    metrics= [\"accuracy\"]  \n",")\n","\n","# Fit\n","history = classifier.fit(x=X_train,\n","                         y=y_train,\n","                         batch_size=BATCH_SIZE,\n","                         epochs=EPOCHS, \n","                         validation_data=(X_val, y_val)\n","                        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:04:19.91385Z","iopub.status.busy":"2023-07-31T20:04:19.913491Z","iopub.status.idle":"2023-07-31T20:04:19.923256Z","shell.execute_reply":"2023-07-31T20:04:19.922094Z","shell.execute_reply.started":"2023-07-31T20:04:19.913815Z"},"trusted":true},"outputs":[],"source":["def displayConfusionMatrix(y_true, y_pred, dataset):\n","    disp = ConfusionMatrixDisplay.from_predictions(\n","        y_true,\n","        np.argmax(y_pred, axis=1),\n","        display_labels=[\"Not Disaster\",\"Disaster\"],\n","        cmap=plt.cm.Blues\n","    )\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n","    f1_score = tp / (tp+((fn+fp)/2))\n","\n","    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:04:19.925162Z","iopub.status.busy":"2023-07-31T20:04:19.924738Z","iopub.status.idle":"2023-07-31T20:05:04.197288Z","shell.execute_reply":"2023-07-31T20:05:04.19632Z","shell.execute_reply.started":"2023-07-31T20:04:19.925118Z"},"trusted":true},"outputs":[],"source":["y_pred_train = classifier.predict(X_train)\n","\n","displayConfusionMatrix(y_train, y_pred_train, \"Training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:05:04.199632Z","iopub.status.busy":"2023-07-31T20:05:04.198688Z","iopub.status.idle":"2023-07-31T20:05:12.995278Z","shell.execute_reply":"2023-07-31T20:05:12.994338Z","shell.execute_reply.started":"2023-07-31T20:05:04.199591Z"},"trusted":true},"outputs":[],"source":["y_pred_val = classifier.predict(X_val)\n","\n","displayConfusionMatrix(y_val, y_pred_val, \"Validation\")"]},{"cell_type":"markdown","metadata":{},"source":["# Generate the submission file \n","\n","For each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n","\n","The `submission.csv` file uses the following format:\n","`id,target`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:05:13.000332Z","iopub.status.busy":"2023-07-31T20:05:12.999248Z","iopub.status.idle":"2023-07-31T20:05:13.023512Z","shell.execute_reply":"2023-07-31T20:05:13.022432Z","shell.execute_reply.started":"2023-07-31T20:05:13.000292Z"},"trusted":true},"outputs":[],"source":["sample_submission = pd.read_csv(\"sample_submission.csv\")\n","sample_submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:05:13.025972Z","iopub.status.busy":"2023-07-31T20:05:13.025189Z","iopub.status.idle":"2023-07-31T20:05:28.765372Z","shell.execute_reply":"2023-07-31T20:05:28.764404Z","shell.execute_reply.started":"2023-07-31T20:05:13.025933Z"},"trusted":true},"outputs":[],"source":["sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:05:28.767064Z","iopub.status.busy":"2023-07-31T20:05:28.766701Z","iopub.status.idle":"2023-07-31T20:05:28.787427Z","shell.execute_reply":"2023-07-31T20:05:28.786399Z","shell.execute_reply.started":"2023-07-31T20:05:28.767029Z"},"trusted":true},"outputs":[],"source":["sample_submission.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-31T20:05:28.789378Z","iopub.status.busy":"2023-07-31T20:05:28.788974Z","iopub.status.idle":"2023-07-31T20:05:28.803925Z","shell.execute_reply":"2023-07-31T20:05:28.803021Z","shell.execute_reply.started":"2023-07-31T20:05:28.78933Z"},"trusted":true},"outputs":[],"source":["sample_submission.to_csv(\"submission.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
